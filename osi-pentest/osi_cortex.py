import streamlit as st
import requests
import json
import socket

# Configuration
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL = "llama3"

st.set_page_config(page_title="OSI Cortex", page_icon="ðŸ§ ", layout="wide")

# Styling
st.markdown("""
<style>
    .stApp {
        background-color: #0e1117;
        color: #00ff41;
    }
    .stTextInput input {
        color: #00ff41 !important;
        border: 1px solid #00ff41 !important;
    }
    div.stMarkdown {
        font-family: 'Courier New', Courier, monospace;
    }
</style>
""", unsafe_allow_html=True)

# Header
st.title("OSI CORTEX // PRIVATE INTELLIGENCE")
st.markdown("---")

# Sidebar for System Status
with st.sidebar:
    st.header("SYSTEM STATUS")
    try:
        # Check Ollama
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        result = sock.connect_ex(('localhost', 11434))
        if result == 0:
            st.success("NEURAL ENGINE: ONLINE")
        else:
            st.error("NEURAL ENGINE: OFFLINE")
        sock.close()
    except:
        st.error("CONNECTION ERROR")
        
    st.markdown("---")
    rag_enabled = st.toggle("Activate RAG Memory", value=False)
    st.markdown("**Security Level:** AIR-GAPPED")
    st.markdown("**Model:** Llama-3-8B")

# Init RAG if needed
if rag_enabled and "rag_engine" not in st.session_state:
    try:
        from osi_rag import RAGBrain
        st.session_state.rag_engine = RAGBrain()
        st.sidebar.success("Memory Loaded")
    except Exception as e:
        st.sidebar.error(f"RAG Load Fail: {e}")

# ... (Chat Loop)

if prompt := st.chat_input("Enter command or query..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # RAG Retrieval
    context_data = ""
    if rag_enabled and "rag_engine" in st.session_state:
        try:
            with st.status("Accessing Corporate Memory...", expanded=False):
                docs = st.session_state.rag_engine.query(prompt)
                context_data = "\n\nRELEVANT CORPORATE DATA:\n" + "\n---\n".join(docs)
                st.write(docs)
        except Exception as e:
            st.error(f"Memory Access Error: {e}")

    # Generate Response
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # Construct Prompt with Context
            final_prompt = prompt
            if context_data:
                final_prompt = f"Using the following corporate data, answer the user query.\n{context_data}\n\nUser Query: {prompt}"
            
            payload = {
                "model": MODEL,
                "prompt": final_prompt,
                "stream": True 
            }
            
            response = requests.post(OLLAMA_URL, json=payload, stream=True)
            
            for line in response.iter_lines():
                if line:
                    body = json.loads(line)
                    if "response" in body:
                        token = body["response"]
                        full_response += token
                        message_placeholder.markdown(full_response + "â–Œ")
                        
            message_placeholder.markdown(full_response)
        except Exception as e:
            st.error(f"Error: {e}")

    # Add assistant response to history
    st.session_state.messages.append({"role": "assistant", "content": full_response})

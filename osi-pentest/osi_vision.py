#!/usr/bin/env python3
import cv2
import sys
import time
import argparse
import numpy as np
from datetime import datetime
from rich.console import Console
from rich.panel import Panel

# Lazy imports to speed up CLI start if dependencies aren't needed yet
try:
    from ultralytics import YOLO
    from sentence_transformers import SentenceTransformer
    import chromadb
except ImportError:
    pass

console = Console()

class OSI_Vision_Engine:
    def __init__(self, source=0, headless=False):
        self.source = source
        self.headless = headless
        self.db_path = "./osi_vision_db"
        
        console.print("[bold green]Initializing OSI Vision Engine...[/bold green]")
        
        # 1. Load Object Detector (YOLOv8 Nano for speed)
        self.detector = YOLO('yolov8n.pt') 
        
        # 2. Load Semantic Encoder (CLIP for text<->image matching)
        # using 'clip-ViT-B-32' which is standard for this.
        console.print("[yellow]Loading CLIP Model (this may take a moment)...[/yellow]")
        self.encoder = SentenceTransformer('clip-ViT-B-32')
        
        # 3. Initialize Vector DB (Chroma)
        self.chroma_client = chromadb.PersistentClient(path=self.db_path)
        self.collection = self.chroma_client.get_or_create_collection(name="surveillance_logs")
        
        console.print("[bold green]System Online.[/bold green]")

    def process_stream(self):
        """Main loop: Capture -> Detect -> Encode -> Store"""
        cap = cv2.VideoCapture(self.source)
        if not cap.isOpened():
            console.print(f"[red]Error: Could not open video source {self.source}[/red]")
            return

        frame_count = 0
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # Run Detection every frame (or skip for performance)
                if frame_count % 5 == 0: # Process every 5th frame
                    results = self.detector(frame, verbose=False)
                    
                    for r in results:
                        boxes = r.boxes
                        for box in boxes:
                            # If confidence > 0.5
                            if box.conf[0] > 0.5:
                                x1, y1, x2, y2 = map(int, box.xyxy[0])
                                cls = int(box.cls[0])
                                label = self.detector.names[cls]
                                
                                # Only care about People for now
                                if label == 'person':
                                    # Extract crop
                                    crop = frame[y1:y2, x1:x2]
                                    if crop.size > 0:
                                        # Encode the crop (Image -> Vector)
                                        # Note: SentenceTransformer handles PIL images best, 
                                        # so we convert numpy -> PIL
                                        from PIL import Image
                                        pil_img = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))
                                        
                                        # Create embedding
                                        vector = self.encoder.encode(pil_img)
                                        
                                        # Store in DB
                                        timestamp = datetime.now().isoformat()
                                        self.collection.add(
                                            embeddings=[vector.tolist()],
                                            documents=[f"Person detected at {timestamp}"],
                                            metadatas=[{"timestamp": timestamp, "camera": str(self.source)}],
                                            ids=[f"det_{timestamp}_{frame_count}"]
                                        )
                                        
                                        # Draw visual box
                                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                                        cv2.putText(frame, f"Logged {label}", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

                if not self.headless:
                    cv2.imshow('OSI Vision - Sentinel Mode', frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
        except KeyboardInterrupt:
            pass
        finally:
            cap.release()
            cv2.destroyAllWindows()
            console.print("[yellow]Stream stopped.[/yellow]")

    def search_memory(self, query, n_results=3):
        """Search the video logs using natural language."""
        console.print(f"\n[bold cyan]Searching Video Memory for: '{query}'...[/bold cyan]")
        
        # 1. Encode text query -> Vector
        query_vec = self.encoder.encode(query)
        
        # 2. Query Memory
        results = self.collection.query(
            query_embeddings=[query_vec.tolist()],
            n_results=n_results
        )
        
        # 3. Display Results
        msg = ""
        for i, meta in enumerate(results['metadatas'][0]):
            msg += f"- [bold]{meta['timestamp']}[/bold]: Match Score {results['distances'][0][i]:.4f}\n"
        
        console.print(Panel(msg, title="Search Results", border_style="green"))

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="OSI Vision - AI Surveillance System")
    parser.add_argument("--mode", choices=['record', 'search'], default='record', help="Mode: record (live stream) or search (query logs)")
    parser.add_argument("--query", help="Natural language query for search mode")
    parser.add_argument("--source", default=0, help="Video source (0 for webcam, or filename)")
    parser.add_argument("--headless", action="store_true", help="Run without UI window")
    
    args = parser.parse_args()
    
    # Check if source is digit (webcam index)
    src = int(args.source) if str(args.source).isdigit() else args.source
    
    engine = OSI_Vision_Engine(source=src, headless=args.headless)
    
    if args.mode == 'record':
        engine.process_stream()
    elif args.mode == 'search':
        if not args.query:
            console.print("[red]Error: --query required for search mode[/red]")
        else:
            engine.search_memory(args.query)

#!/usr/bin/env python3
import os
import glob
import chromadb
from rich.console import Console
from rich.progress import track

# Text Extractors
import pypdf
import docx2txt

# Embedding
from sentence_transformers import SentenceTransformer

console = Console()

class RAGBrain:
    def __init__(self, db_path="./osi_vision_db"): # Shared DB path
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2') # Faster text model
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(name="corporate_memory")
        
    def extract_text(self, filepath):
        ext = filepath.split('.')[-1].lower()
        try:
            if ext == 'pdf':
                reader = pypdf.PdfReader(filepath)
                text = ""
                for page in reader.pages:
                    text += page.extract_text() + "\n"
                return text
            elif ext == 'docx':
                return docx2txt.process(filepath)
            elif ext == 'txt' or ext == 'md':
                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
        except Exception as e:
            console.print(f"[red]Error reading {filepath}: {e}[/red]")
            return ""
        return ""

    def ingest_directory(self, directory):
        console.print(f"[bold cyan]Scanning {directory} for documents...[/bold cyan]")
        files = []
        for ext in ['pdf', 'docx', 'txt', 'md']:
            files.extend(glob.glob(os.path.join(directory, f"**/*.{ext}"), recursive=True))
            
        console.print(f"Found {len(files)} documents.")
        
        for fpath in track(files, description="Ingesting Knowledge..."):
            text = self.extract_text(fpath)
            if not text: continue
            
            # Simple chunking (500 chars)
            chunks = [text[i:i+500] for i in range(0, len(text), 500)]
            
            emb_list = self.encoder.encode(chunks).tolist()
            ids = [f"{os.path.basename(fpath)}_{i}" for i in range(len(chunks))]
            metadatas = [{"source": fpath, "chunk": i} for i in range(len(chunks))]
            
            self.collection.add(
                embeddings=emb_list,
                documents=chunks,
                ids=ids,
                metadatas=metadatas
            )
            
        console.print("[bold green]Ingestion Complete. Memory Updated.[/bold green]")

    def query(self, text, n_results=3):
        vec = self.encoder.encode(text).tolist()
        res = self.collection.query(query_embeddings=[vec], n_results=n_results)
        return res['documents'][0]

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("directory", help="Directory to ingest")
    args = parser.parse_args()
    
    rag = RAGBrain()
    rag.ingest_directory(args.directory)
